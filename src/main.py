
import re, sys, unicodedata, json, os, datetime
from pathlib import Path
import torch
from modelroketsan import GPTLanguageModel

RAW_PATH       = '/content/input.txt'
PROP_PATH      = '/content/veri_properties.txt'
HISTORY_PATH   = '/content/chat_history.jsonl'
EXPORT_MD_PATH = '/content/chat_history.md'

# DavranÄ±ÅŸ ayarlarÄ±
STRICT_DEFINITION = True   # "nedir?" tÃ¼rÃ¼ sorularda tanÄ±m yoksa Ã¶zelliklere dÃ¼ÅŸme

# --- Veri yÃ¼kleme (input.txt) ---
raw = Path(RAW_PATH).read_text(encoding='utf-8').strip()

# Orijinal terim -> tanÄ±m sÃ¶zlÃ¼ÄŸÃ¼
d = {}
for blk in re.split(r'\n{2,}', raw):
    if ':' in blk:
        term, defn = blk.split(':', 1)
        d[term.strip()] = ' '.join(defn.strip().splitlines())

# --- Normalizasyon yardÄ±mcÄ±larÄ± ---
_TMAP = str.maketrans({
    'Ä±':'i','Ä°':'i','I':'i','Ã§':'c','Ã‡':'c','ÄŸ':'g','Ä':'g',
    'Ã¶':'o','Ã–':'o','ÅŸ':'s','Å':'s','Ã¼':'u','Ãœ':'u'
})
def norm(s: str) -> str:
    s = s.strip().casefold().translate(_TMAP)
    s = unicodedata.normalize('NFKD', s)
    return ''.join(ch for ch in s if not unicodedata.combining(ch))

def norm_squash(s: str) -> str:
    s = norm(s)
    return re.sub(r'[^0-9a-z]+', '', s)

def canon_name(name: str) -> str:
    return re.sub(r'\s*\(.+?\)\s*', '', name).strip()

# --- Alias Ã§Ä±karÄ±mÄ±: baÅŸlÄ±ktan varyantlar Ã¼ret ---
def make_aliases(term: str):
    aliases = set()
    term_clean = re.sub(r'\s+', ' ', term or '').strip()
    if term_clean:
        aliases.add(term_clean)

    m = re.search(r'^(.*?)\s*\(([^()]*)\)\s*$', term_clean)
    if m:
        a1 = m.group(1).strip()
        a2 = m.group(2).strip()
        if a1: aliases.add(a1)
        if a2: aliases.add(a2)

    for sep in ['/', '-', 'â€“', 'â€”', ':', '_']:
        if sep in term_clean:
            for p in (x.strip() for x in term_clean.split(sep)):
                if len(p) >= 2:
                    aliases.add(p)

    spaced = re.sub(r'[-â€“â€”/_]+', ' ', term_clean).strip()
    if spaced:
        aliases.add(spaced)

    squashed = re.sub(r'[\s\-â€“â€”/_:()]+', '', term_clean)
    if squashed:
        aliases.add(squashed)

    DROP = {'a','o','u','i','of','and','the'}
    aliases = {a for a in aliases if len(norm(a)) >= 2 and norm(a) not in DROP}
    return aliases

# --- veri_properties.txt oku ---
props = {}        # adÄ± -> { Alan:..., TÃ¼r:..., MenÅŸei:..., Ã–zellikler:..., ... }
props_text = {}   # adÄ± -> tek satÄ±r Ã¶zet (arama iÃ§in)
# Oturum iÃ§i Ã¶ÄŸrenilen Ã¶zellikler (resmi kaynaÄŸÄ±n Ã¼stÃ¼ne yazmaz; sadece eksik yerleri tamamlar)
learned_props = {}  # adÄ± -> { Alan:..., TÃ¼r:..., MenÅŸei:..., Ã–zellikler:..., ... }

if os.path.exists(PROP_PATH):
    ptxt = Path(PROP_PATH).read_text(encoding='utf-8').strip()
    blocks, cur = [], []
    for line in ptxt.splitlines():
        if line.strip().endswith('):') and '(' in line:
            if cur:
                blocks.append('\n'.join(cur).strip())
                cur = []
        cur.append(line)
    if cur:
        blocks.append('\n'.join(cur).strip())

    def flush_current(k, buf, rec):
        if k is not None:
            rec[k] = ' '.join(' '.join(buf).split())
        return None, []

    for blk in blocks:
        lines = [l.rstrip() for l in blk.splitlines() if l.strip()]
        head = lines[0]
        name = head[:-1].strip()
        rec = {}
        k = None
        buf = []
        for l in lines[1:]:
            m = re.match(r'^([A-Za-zÃ‡ÄÄ°Ã–ÅÃœÃ§ÄŸÄ±Ã¶ÅŸÃ¼\s/()-]+):\s*(.*)$', l)
            if m:
                k, buf = flush_current(k, buf, rec)
                k = m.group(1).strip()
                val = m.group(2).strip()
                buf = [val] if val else []
            else:
                buf.append(l.strip())
        k, buf = flush_current(k, buf, rec)
        props[name] = rec
        props_text[name] = '; '.join(f"{kk}: {vv}" for kk, vv in rec.items())

# === PATCH: Ã–zellik etiketleri + alias + feature->terms indeksi ===
def split_features(val):
    if not val:
        return []
    if isinstance(val, list):
        items = val
    else:
        items = re.split(r',|;|\n|\||/|â€¢', val)
    return [i.strip(' .;') for i in items if i and i.strip()]

def add_rule_based_tags_inplace(props_dict):
    for name, rec in props_dict.items():
        # TÃ¼rkÃ§e karakter/aksan normalizasyonuyla tek gÃ¶vde
        blob_parts = []
        for k, v in rec.items():
            blob_parts.append(str(v))
        blob = norm(" ".join(blob_parts))

        feats = set(split_features(rec.get('Ã–zellikler', '')))

        # ---- A2G (Havadan Karaya): hava platform + kara hedef; hava savunma sistemlerini hariÃ§ tut
        is_air_defense = any(k in blob for k in [
            'hava savunma','ciws','korkut','tehditlere karsi','tehditlerine karsi'
        ])
        a2g_platform = any(k in blob for k in [
            'ucaktan atilan','hava platform','helikopter','iha','siha',
            'ucak entegrasyonu','helikopter entegrasyonu','iha/siha uyumu','iha siha uyumu'
        ])
        a2g_ground = any(k in blob for k in ['kara hedef','yer hedef','tanksavar'])
        if a2g_platform and a2g_ground and not is_air_defense:
            feats.add('Havadan Karaya')

        # A2A (Havadan Havaya)
        if any(k in blob for k in ['hava-hava','hava hava','wvr','bvr']):
            feats.add('Havadan Havaya')

        # S2A (Karadan Havaya) - hava savunma/manpads/HÄ°SAR vb.
        if any(k in blob for k in ['hava savunma','manpads','alcak irtifa','orta menzil','uzun menzil']) \
           and any(k in blob for k in ['fuze','sistem']):
            feats.add('Karadan Havaya')

        # S2S (Karadan Karaya): balistik, Ã‡NRA vb.
        if any(k in blob for k in ['balistik fuze','cok namlulu roketatar','cnra']) or ('balistik' in blob and 'fuze' in blob):
            feats.add('Karadan Karaya')

        # Denizden Denize
        if any(k in blob for k in ['gemisavar','deniz hedef','ciws','torpido','deniz platform']):
            feats.add('Denizden Denize')

        # Omuzdan AtÄ±lan
        if any(k in blob for k in ['omuzdan atilan','tek personel tarafindan tasinabilir','manpads']):
            feats.add('Omuzdan AtÄ±lan')

        # Tanksavar
        if 'tanksavar' in blob or 'zirh delici' in blob:
            feats.add('Tanksavar')

        # Gemisavar etiketi
        if any(k in blob for k in ['gemisavar','anti-ship','ashm']):
            feats.add('Gemisavar')

        if feats:
            rec['Ã–zellikler'] = ', '.join(sorted(feats))

add_rule_based_tags_inplace(props)

FEATURE_ALIASES = {
    'havadan karaya': ['hava-yer','hava yer','air-to-ground','a2g','havadan karaya fÃ¼ze','havadan karaya fÃ¼zeler'],
    'havadan havaya': ['hava-hava','hava hava','air-to-air','a2a'],
    'karadan havaya': ['yerden havaya','surface-to-air','s2a','hava savunma','hava savunma fÃ¼zesi'],
    'karadan karaya': ['yerden yere','surface-to-surface','s2s','kara-kara'],
    'denizden denize': ['sea-to-sea','gemisavar','anti-ship'],
    'omuzdan atÄ±lan': ['omuzdan atilabilen','portable','manpads'],
    'tanksavar': ['atgm','anti-tank','tank savar'],
    'gemisavar': ['anti-ship','ashm'],
}

FEATURE_DEFINITIONS = {
    'havadan karaya' : "Hava platformlarÄ±ndan kara/yer hedeflerine atÄ±lan gÃ¼dÃ¼mlÃ¼ mÃ¼himmat ailesi (A2G).",
    'havadan havaya' : "Hava platformlarÄ±ndan hava hedeflerine karÅŸÄ± kullanÄ±lan gÃ¼dÃ¼mlÃ¼ fÃ¼zeler (A2A).",
    'karadan havaya' : "Yer konuÅŸlu hava savunma sistemleri; hava hedeflerini Ã¶nler (S2A).",
    'karadan karaya' : "Yer konuÅŸlu sistemlerden yine yer hedeflerine atÄ±lan roket/fÃ¼zeler (S2S).",
    'denizden denize' : "Deniz platformlarÄ±ndan deniz hedeflerine (gemi/denizaltÄ±) karÅŸÄ± silahlar.",
    'omuzdan atÄ±lan' : "Tek personelce taÅŸÄ±nÄ±p atÄ±labilen (MANPADS/ATGM) hafif gÃ¼dÃ¼mlÃ¼ silahlar.",
    'tanksavar'      : "ZÄ±rhlÄ± hedefleri imha etmek iÃ§in tasarlanmÄ±ÅŸ gÃ¼dÃ¼mlÃ¼ mÃ¼himmat ailesi.",
    'gemisavar'      : "Deniz hedeflerine karÅŸÄ± kullanÄ±lan, Ã§oÄŸunlukla aktif radar/IR gÃ¼dÃ¼mlÃ¼ fÃ¼zeler.",
}

# Ã–zellik -> terimler (alias'larla birlikte)
feature2terms = {}
for name, rec in props.items():
    feats = split_features(rec.get('Ã–zellikler', ''))
    base = canon_name(name)
    for f in feats:
        nf = norm(f)
        if not nf:
            continue
        feature2terms.setdefault(nf, set()).add(base)
        for root, vars_ in FEATURE_ALIASES.items():
            if nf == norm(root):
                for var in vars_:
                    feature2terms.setdefault(norm(var), set()).add(base)

# --- EÅŸleÅŸme dizinleri (tanÄ±m + Ã¶zellik baÅŸlÄ±klarÄ±ndan) ---
all_terms = sorted(set(list(d.keys()) + list(props.keys())))
alias2term = {}
alias2term_squash = {}
for term in all_terms:
    for a in make_aliases(term):
        alias2term[norm(a)] = term
        alias2term_squash[norm_squash(a)] = term

ALIAS_NORM_DESC   = sorted(alias2term.keys(), key=len, reverse=True)
ALIAS_SQUASH_DESC = sorted(alias2term_squash.keys(), key=len, reverse=True)

# === Dinamik Ã¶ÄŸrenme yardÄ±mcÄ±larÄ± (Ã¶ÄŸretme) ===
def rebuild_alias_indexes():
    global all_terms, alias2term, alias2term_squash, ALIAS_NORM_DESC, ALIAS_SQUASH_DESC
    all_terms = sorted(set(list(d.keys()) + list(props.keys())))
    alias2term = {}
    alias2term_squash = {}
    for term in all_terms:
        for a in make_aliases(term):
            alias2term[norm(a)] = term
            alias2term_squash[norm_squash(a)] = term
    ALIAS_NORM_DESC   = sorted(alias2term.keys(), key=len, reverse=True)
    ALIAS_SQUASH_DESC = sorted(alias2term_squash.keys(), key=len, reverse=True)

def persist_new_definition(term: str, definition: str):
    block = f"\n\n{term}: {definition}\n"
    # 1) Ã–ncelik: RAW_PATH
    try:
        with open(RAW_PATH, 'a', encoding='utf-8') as f:
            f.write(block)
        return True, RAW_PATH
    except Exception:
        pass
    # 2) Alternatif: script ile aynÄ± klasÃ¶rdeki input.txt
    try:
        local = Path(__file__).resolve().parent / 'input.txt'
        with open(local, 'a', encoding='utf-8') as f:
            f.write(block)
        return True, str(local)
    except Exception:
        return False, None

def teach_definition_inline(text: str):
    # Beklenen biÃ§im: "TERÄ°M: <TanÄ±m veya Alan: DeÄŸer; Ã–zellikler: a,b; ...>"
    if ':' not in text:
        return False, "BiÃ§im: /ogret Terim: TanÄ±m | veya /ogret Terim: TÃ¼r: X; MenÅŸei: Y; Ã–zellikler: a, b"
    term, payload = text.split(':', 1)
    term = term.strip()
    payload = payload.strip()
    if not term or not payload:
        return False, "BiÃ§im: /ogret Terim: TanÄ±m"
    # Alan anahtarlarÄ± var mÄ±?
    has_field = any(k in payload for k in ['TÃ¼r:', 'MenÅŸei:', 'Ã–zellikler:', 'Ozellikler:', 'TanÄ±m:', 'Tanim:'])
    if has_field:
        # satÄ±r/; ayrÄ±mÄ±yla parÃ§ala
        kvs = re.split(r';|\n', payload)
        for kv in kvs:
            if ':' not in kv:
                continue
            k, v = kv.split(':', 1)
            k = k.strip()
            v = v.strip()
            key_norm = k.lower()
            if key_norm in ('tanÄ±m', 'tanim'):
                d[term] = v
                ok, _ = persist_new_definition(term, v)
            elif key_norm in ('Ã¶zellikler', 'ozellikler'):
                set_user_field(term, 'Ã–zellikler', [s.strip() for s in re.split(r',|\|', v) if s.strip()])
            elif key_norm == 'tÃ¼r':
                set_user_field(term, 'TÃ¼r', v)
            elif key_norm in ('menÅŸei', 'mensei', 'menÅŸe', 'koken', 'kÃ¶ken', 'origin'):
                set_user_field(term, 'MenÅŸei', v)
        rebuild_alias_indexes()
        return True, f" Ã–ÄŸrenildi: {term} (oturum geÃ§erli; tanÄ±m varsa dosyaya eklendi)"
    else:
        # Sade tanÄ±m
        d[term] = payload
        rebuild_alias_indexes()
        ok, path = persist_new_definition(term, payload)
        if ok:
            return True, f" Ã–ÄŸrenildi ve kaydedildi: {term} (dosya: {path})"
        else:
            return True, f" Ã–ÄŸrenildi (oturum iÃ§i). Kaydetme baÅŸarÄ±sÄ±z: {term}"

def summarize_types(feature_filter_root: str | None = None):
    # TÃ¼r alanlarÄ±nÄ± topla; feature filtre varsa yalnÄ±z o Ã¶zelliÄŸe sahip kayÄ±tlar
    types = []
    for name, rec in props.items():
        if feature_filter_root:
            feats = split_features(rec.get('Ã–zellikler', ''))
            keys = [norm(feature_filter_root)] + [norm(s) for s in FEATURE_ALIASES.get(feature_filter_root, [])]
            if not any(norm(f) in keys for f in feats):
                continue
        t = rec.get('TÃ¼r')
        if t:
            types.append(t.strip())
    # benzersiz, tutarlÄ± sÄ±ra
    uniq = []
    for t in types:
        if t not in uniq:
            uniq.append(t)
    lines = [f"TÃ¼r SayÄ±sÄ±: {len(uniq)}", "TÃ¼rler:"]
    for i, t in enumerate(uniq, 1):
        lines.append(f"{i}. {t}")
    return "\n".join(lines)

# --- Soru kalÄ±plarÄ± ve noktalama ---
QUESTION_TAIL_RE = re.compile(
    r'\b('
    r'nedir|ne|ne demek|ne anlama gelir|kimdir|neredir|nedir acaba|'
    r'tanimi|tanÄ±mÄ±|aciklamasi|aÃ§Ä±klamasÄ±|hakkinda bilgi|hakkÄ±nda bilgi'
    r')\b\??',
    flags=re.IGNORECASE
)
PUNCT_RE = re.compile(r'[?!.,;:â€¦â€œâ€"\'`Â´^~â€¢Â·()$begin:math:display$$end:math:display${}<>]+')

# --- Alan/niyet sÃ¶zlÃ¼ÄŸÃ¼ ---
FIELD_SYNS = {
    'Ã–zellikler': ['ozellik','ozellikleri','Ã¶zellik','Ã¶zellikleri','ozellikler','Ã¶zellikler','ozelligi','Ã¶zelliÄŸi','detay','kabiliyet','kabiliyetleri','tumu','hepsi'],
    'TÃ¼r'       : ['tur','turu','tÃ¼r','tÃ¼rÃ¼','kategori','type','sinif','sÄ±nÄ±f'],
    'MenÅŸei'    : ['mensei','menÅŸei','menÅŸe','origin','koken','kÃ¶ken'],
    'KuruluÅŸ'   : ['kurulus','kuruluÅŸ','yil','yÄ±lÄ±','kurulma','kurulus yili','yÄ±l'],
    'Alan'      : ['alan','faaliyet alan','sektor','sektÃ¶r','uzmanlik','uzmanlÄ±k','alanÄ±'],
    'KullanÄ±m'  : ['kullanim','kullanÄ±m','kullanim amaci','kullanim alan','rol','rolu','rolÃ¼'],
    'Ãœretici'   : ['uretici','Ã¼retici','ureten','manufacturer','yapan'],
}
LIST_HINTS  = ['hangileri','kimlerde','kimler','olanlar','iÃ§eren','iÃ§erenler','sahip olanlar','listesi','hangi','hepsi']
COUNT_HINTS = ['kaÃ§','kac','kaÃ§ tane','kac tane','sayisi','sayÄ±sÄ±','adet','sayi','sayÄ±','how many','count','number']
TYPE_HINTS  = ['tÃ¼r','tur','tÃ¼rler','tÃ¼rleri','turler','turleri']

# --- SÄ±nÄ±flandÄ±rma (kÃ¼resel) ipuÃ§larÄ± ---
CLASSIFY_HINTS = [
    'siniflandir','sÄ±nÄ±flandÄ±r','siniflandirma','sÄ±nÄ±flandÄ±rma',
    'gruplandir','gruplandÄ±r','gruplama',
    'kategoriye gore','kategoriye gÃ¶re','kategori bazli','kategori bazlÄ±',
    'ozelliklere gore','Ã¶zelliklere gore','Ã¶zelliklere gÃ¶re','ozellik bazli','Ã¶zellik bazlÄ±',
    'etiketlere gore','etiketlere gÃ¶re','kategori dagilimi','kategori daÄŸÄ±lÄ±mÄ±','dagilim','daÄŸÄ±lÄ±m'
]

def wants_count(q: str) -> bool:
    n = norm(q)
    return any(norm(h) in n for h in COUNT_HINTS)

def wants_type_group(q: str) -> bool:
    n = norm(q)
    return any(norm(h) in n for h in TYPE_HINTS) and (wants_count(q) or any(h in n for h in ['tÃ¼rler','turler','tÃ¼rleri','turleri']))

# --- Coreference patch: Ã¶nceki terime gÃ¶nderme tespiti ---
COREF_PRONOUNS = r'\b(bu|ÅŸu|o|bunlar|ÅŸunlar|onlar|bunun|ÅŸunun|onun|buna|ÅŸuna|ona|bundan|ÅŸundan|ondan)\b'
def is_coref_like(q: str) -> bool:
    n = norm(q)
    # Zamir / iÅŸaret sÃ¶zcÃ¼kleri
    if re.search(COREF_PRONOUNS, ' ' + q.lower() + ' '):
        return True
    # Alan/niyet kelimeleri veya liste/sayÄ±m veya tanÄ±m kalÄ±bÄ±
    field_hit = any(norm(s) in n for syns in FIELD_SYNS.values() for s in syns) or any(
        k in n for k in ['Ã¶zellik','ozellik','tÃ¼r','tur','menÅŸei','mensei'])
    list_or_count = wants_count(q) or any(norm(h) in n for h in LIST_HINTS)
    return field_hit or list_or_count or is_definition_query(q)

# --- Ã‡oklu terim Ã§Ä±karÄ±mÄ± (aday listesi) ---
def extract_terms_raw(q: str):
    """Sorgudaki TÃœM terimleri (d/props baÅŸlÄ±klarÄ±) dÃ¶ndÃ¼r (tekrarlarÄ± at)."""
    n_keep = norm(q)
    n_sq   = norm_squash(q)
    found = []
    seen = set()
    for key in ALIAS_NORM_DESC:
        if key and key in n_keep:
            t = alias2term[key]
            if t not in seen:
                seen.add(t); found.append(t)
    for key in ALIAS_SQUASH_DESC:
        if key and key in n_sq:
            t = alias2term_squash[key]
            if t not in seen:
                seen.add(t); found.append(t)
    if found:
        return found
    # --- Fuzzy eÅŸleÅŸme (Ã¶r. MAM-L ~ maml) ---
    def _lev(a: str, b: str, max_d: int = 1) -> int:
        # kÃ¼Ã§Ã¼k ve hÄ±zlÄ± Levenshtein (erken kesme)
        if abs(len(a) - len(b)) > max_d:
            return max_d + 1
        dp = list(range(len(b) + 1))
        for i, ca in enumerate(a, 1):
            prev = dp[0]
            dp[0] = i
            best = dp[0]
            for j, cb in enumerate(b, 1):
                cur = dp[j]
                cost = 0 if ca == cb else 1
                dp[j] = min(dp[j] + 1, dp[j - 1] + 1, prev + cost)
                prev = cur
                if dp[j] < best: best = dp[j]
            if best > max_d:
                return max_d + 1
        return dp[-1]
    toks = [t for t in re.split(r'\s+', n_sq) if len(t) >= 3]
    for tok in toks:
        # alias2term_squash Ã¼zerinden yakÄ±n eÅŸleÅŸme ara
        for key, term in alias2term_squash.items():
            if len(key) >= 3 and _lev(tok, key, max_d=1) <= 1:
                if term not in seen:
                    seen.add(term); found.append(term)
        if found:
            break
    # --- Coreference fallback: hiÃ§ terim bulunamadÄ±ysa Ã¶nceki konuya baÄŸlan ---
    if not found and is_coref_like(q):
        last = get_last_subject()
        if last:
            return [last]
    return found

# TanÄ±m sinyali & kategori tespiti
DEF_HINTS = ['nedir','ne demek','ne anlama gelir','kimdir','neredir','tanimi','tanÄ±mÄ±','aciklamasi','aÃ§Ä±klamasÄ±','kisaca acikla','kÄ±saca aÃ§Ä±kla',' ne ']
def is_definition_query(q: str) -> bool:
    n = f" {norm(q)} "
    if ' ne ' in n:
        terms = extract_terms_raw(q)
        if terms:
            return True
    return any(f" {h} " in n for h in DEF_HINTS if h != ' ne ') or (' nedir ' in n)

def extract_feature_from_query(q: str):
    n = norm(q)
    for root, syns in FEATURE_ALIASES.items():
        keys = [norm(root)] + [norm(s) for s in syns]
        if any(k in n for k in keys):
            return root
    return None

# --- Sorgudan tek terim Ã§Ä±kar (tek varlÄ±k gerektiren yanÄ±tlar iÃ§in) ---
def extract_term(q: str):
    terms = extract_terms_raw(q)
    if len(terms) == 1:
        return terms[0]
    return None  # 0 veya 2+

# --- Gruplama yardÄ±mcÄ±larÄ± (Ã¶zellik kÃ¶klerine gÃ¶re) ---
def terms_for_feature_root(root: str):
    keys = [norm(root)] + [norm(s) for s in FEATURE_ALIASES.get(root, [])]
    items = set()
    for k in keys:
        items |= feature2terms.get(k, set())
    return sorted(items)

def summarize_feature_counts(include_lists: bool):
    lines = []
    total_unique = set()
    roots = list(FEATURE_ALIASES.keys())
    for r in roots:
        items = terms_for_feature_root(r)
        total_unique |= set(items)
        if include_lists and items:
            lines.append(f" - {r.title()}: {len(items)}\n   " + ", ".join(items))
        else:
            lines.append(f" - {r.title()}: {len(items)}")
    lines.append(f"\n Genel toplam (benzersiz Ã¶ÄŸe): {len(total_unique)}")
    return "\n".join(lines)

# --- Niyet tespiti (Ã–NCE sÄ±nÄ±flandÄ±rma, sonra Ã¶zellik/alan > tanÄ±m > kategori/list) ---
def detect_intent(q: str):
    n = norm(q)

    # 0) Ã–zelliklere gÃ¶re sÄ±nÄ±flandÄ±rma / daÄŸÄ±lÄ±m talebi
    if any(h in n for h in CLASSIFY_HINTS):
        return ('classify', None)

    # 1) AÃ§Ä±k Ã¶zellik/alan sinyali
    for s in FIELD_SYNS['Ã–zellikler']:
        if norm(s) in n:
            return ('features', None)
    for field, syns in FIELD_SYNS.items():
        if field == 'Ã–zellikler':
            continue
        for s in syns + [field]:
            if norm(s) in n:
                return ('field', field)

    # 1.5) TÃ¼r gruplama/sayÄ±m
    if wants_type_group(q):
        return ('type_group', None)

    # 2) TanÄ±m sinyali (ne/nedir ...)
    if is_definition_query(q):
        return ('definition', None)

    # 3) Kategori bazlÄ± liste (COUNT veya LIST varsa ya da def deÄŸilse)
    cat_root = extract_feature_from_query(q)
    if cat_root:
        if any(h in n for h in LIST_HINTS) or any(h in n for h in COUNT_HINTS) or not is_definition_query(q):
            return ('feature_list', cat_root)

    # 4) Genel listeleme
    if any(h in n for h in LIST_HINTS):
        return ('list', None)

    # 5) VarsayÄ±lan: tanÄ±m
    return ('definition', None)

# --- Ã–zellik yardÄ±mcÄ±larÄ± ---
def props_for_entity(name: str):
    if name in props:
        rec = props[name]
        s = '; '.join(f"{k}: {v}" for k, v in rec.items()) if rec else "(kayÄ±t yok)"
        return name, s
    n = norm(name); nsq = norm_squash(name)
    if n in alias2term:
        t = alias2term[n]
        if t in props:
            rec = props[t]
            s = '; '.join(f"{k}: {v}" for k, v in rec.items()) if rec else "(kayÄ±t yok)"
            return t, s
    if nsq in alias2term_squash:
        t = alias2term_squash[nsq]
        if t in props:
            rec = props[t]
            s = '; '.join(f"{k}: {v}" for k, v in rec.items()) if rec else "(kayÄ±t yok)"
            return t, s
    return None, None

def get_field_value(entity: str, field: str):
    """Resmi kaynaktan alan deÄŸeri, yoksa oturum iÃ§i Ã¶ÄŸrenilen deÄŸeri dÃ¶ndÃ¼r.
    Ã‡akÄ±ÅŸma durumunda resmi deÄŸer Ã¶nceliklidir; not olarak kullanÄ±cÄ± bilgisini bildirir.
    Returns: (value, source, conflict_note)
    """
    source = None
    conflict = None
    # entity canonical Ã§Ã¶zÃ¼mle
    if entity not in props:
        n = norm(entity); nsq = norm_squash(entity)
        if n in alias2term and alias2term[n] in props:
            entity = alias2term[n]
        elif nsq in alias2term_squash and alias2term_squash[nsq] in props:
            entity = alias2term_squash[nsq]
    # 1) Resmi
    if entity in props:
        if field in props[entity]:
            return props[entity][field], 'official', None
        tgt = norm(field)
        for k, v in props[entity].items():
            if norm(k) == tgt:
                return v, 'official', None
    # 2) KullanÄ±cÄ± bilgisi
    lp = learned_props.get(entity, {})
    if field in lp:
        # Ã‡akÄ±ÅŸmayÄ± tespit et (resmi varsa ve farklÄ±ysa)
        if entity in props and field in props[entity]:
            if props[entity][field] != lp[field]:
                conflict = f"(Resmi kaynak Ã¶ncelikli; kullanÄ±cÄ± bilgisi farklÄ±: {lp[field]})"
        return lp[field], 'user', conflict
    tgt = norm(field)
    for k, v in lp.items():
        if norm(k) == tgt:
            if entity in props and k in props[entity] and props[entity][k] != v:
                conflict = f"(Resmi kaynak Ã¶ncelikli; kullanÄ±cÄ± bilgisi farklÄ±: {v})"
            return v, 'user', conflict
    return None, None, None

def get_features_list(entity: str):
    """Sadece Ã–zellikler alanÄ±nÄ± liste olarak dÃ¶ndÃ¼r. Resmi yoksa kullanÄ±cÄ± bilgisini kullan."""
    feats_off = None
    feats_user = None
    en = entity
    if en not in props:
        n = norm(en); nsq = norm_squash(en)
        if n in alias2term and alias2term[n] in props:
            en = alias2term[n]
        elif nsq in alias2term_squash and alias2term_squash[nsq] in props:
            en = alias2term_squash[nsq]
    if en in props:
        v = props[en].get('Ã–zellikler')
        if isinstance(v, str):
            feats_off = [s.strip() for s in re.split(r',|;|\n|\||/|â€¢', v) if s.strip()]
        elif isinstance(v, list):
            feats_off = v
    lp = learned_props.get(en, {})
    v2 = lp.get('Ã–zellikler')
    if v2:
        if isinstance(v2, str):
            feats_user = [s.strip() for s in re.split(r',|;|\n|\||/|â€¢', v2) if s.strip()]
        elif isinstance(v2, list):
            feats_user = v2
    return feats_off, feats_user

def set_user_field(entity: str, field: str, value):
    en = entity
    if en not in learned_props:
        learned_props[en] = {}
    learned_props[en][field] = value
    # Ã–zellikler alanÄ± gÃ¼ncellendiyse kategoriler iÃ§in indeksleri tazelemek gerekli olabilir (hafif)
    # Burada sadece alias indeksini tazelemek yeterli
    rebuild_alias_indexes()

def list_by_keyword(q: str):
    n = norm(q)
    t = extract_term(q)
    if t:
        n = n.replace(norm(t), ' ')
    stop = set(['hangileri','kimlerde','kimler','olanlar','olan','var','mi','mÄ±','mu','mÃ¼',
                'listesi','hangi','hepsi','Ã¶zellik','ozellik','ozellikler','Ã¶zellikler','tÃ¼r','tur','menÅŸei','mensei'])
    kws = [w for w in re.split(r'\s+', n) if w and len(w) > 2 and w not in stop]
    if not kws:
        return []
    out = []
    for name, rec in props.items():
        hay = norm(name) + ' ' + norm(' '.join(rec.keys())) + ' ' + norm(' '.join(rec.values()))
        if all(kw in hay for kw in kws):
            out.append(name)
    return sorted(set(out), key=str.lower)

def list_by_feature(q: str):
    n = norm(q)
    hits = set()
    for root, syns in FEATURE_ALIASES.items():
        keys = [norm(root)] + [norm(s) for s in syns]
        if any(k in n for k in keys):
            for k in keys:
                hits |= feature2terms.get(k, set())
    return sorted(hits)

# === GeÃ§miÅŸ (memory + dosya) ===
history = []
save_enabled = True

if os.path.exists(HISTORY_PATH):
    with open(HISTORY_PATH, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                history.append(json.loads(line))
            except json.JSONDecodeError:
                pass

# --- Coreference patch: geÃ§erli konuyu takip et ---
CURRENT_SUBJECT = None  # en son tekil Ã§Ã¶zÃ¼mlenen terim (canon)
def set_current_subject(t: str | None):
    global CURRENT_SUBJECT
    if t:
        CURRENT_SUBJECT = canon_name(t)
def get_last_subject() -> str | None:
    if CURRENT_SUBJECT:
        return CURRENT_SUBJECT
    for rec in reversed(history):
        t = rec.get("term")
        if isinstance(t, str) and t.strip():
            return canon_name(t)
    return None

def now_iso():
    # timezone-aware ISO â†’ Z
    return datetime.datetime.now(datetime.timezone.utc).isoformat(timespec='seconds').replace('+00:00', 'Z')

def log_event(user_msg, term, ans, candidates=None):
    rec = {
        "ts": now_iso(),
        "user": user_msg,
        "term": term if isinstance(term, str) else None,
        "candidates": candidates if isinstance(term, list) else None,
        "answer_found": bool(ans),
        "answer": ans if ans else None
    }
    history.append(rec)
    if save_enabled:
        with open(HISTORY_PATH, 'a', encoding='utf-8') as f:
            f.write(json.dumps(rec, ensure_ascii=False) + '\n')

def print_history(n=20):
    items = history[-n:] if n>0 else history
    if not items:
        print("HenÃ¼z geÃ§miÅŸ yok.\n")
        return
    print(f"\nSon {len(items)} kayÄ±t:")
    for i, rec in enumerate(items, 1):
        tag = "âœ…" if rec.get("answer_found") else ("ğŸ¤”" if rec.get("candidates") else "âŒ")
        if rec.get("candidates"):
            cand = ", ".join(rec["candidates"])
            print(f"{i:02d}. [{rec['ts']}] {tag} Soru: {rec['user']}  â†’ Adaylar: {cand}")
        else:
            print(f"{i:02d}. [{rec['ts']}] {tag} Soru: {rec['user']}")
            if rec.get("term"):
                print(f"    Terim: {rec['term']}")
            if rec.get("answer"):
                print(f"    Cevap: {rec['answer']}")
    print("")

def export_markdown(path=EXPORT_MD_PATH):
    if not history:
        print("â„¹ DÄ±ÅŸa aktarÄ±lacak kayÄ±t yok.")
        return
    lines = ["# Chat GeÃ§miÅŸi\n"]
    for rec in history:
        tag = "âœ…" if rec.get("answer_found") else ("ğŸ¤”" if rec.get("candidates") else "âŒ")
        lines.append(f"## {tag} {rec['ts']}")
        lines.append(f"**Soru:** {rec['user']}")
        if rec.get("candidates"):
            lines.append(f"**Aday Terimler:** {', '.join(rec['candidates'])}")
        if rec.get("term"):
            lines.append(f"**Terim:** {rec['term']}")
        if rec.get("answer"):
            lines.append(f"**Cevap:** {rec['answer']}")
        lines.append("")
    Path(path).write_text("\n".join(lines), encoding='utf-8')
    print(f" Markdown dÄ±ÅŸa aktarÄ±ldÄ±: {path}")

def toggle_save(on: bool):
    global save_enabled
    save_enabled = on
    state = "aÃ§Ä±k" if on else "kapalÄ±"
    print(f" KayÄ±t modu {state}.")

def clear_history():
    global history
    history = []
    try:
        if os.path.exists(HISTORY_PATH):
            os.remove(HISTORY_PATH)
        if os.path.exists(EXPORT_MD_PATH):
            os.remove(EXPORT_MD_PATH)
    except Exception as e:
        print(f" Dosya silinirken hata: {e}")
    print("ğŸ§¹ GeÃ§miÅŸ sÄ±fÄ±rlandÄ±.")

HELP_TEXT = """
ğŸ”§ Komutlar:
  /gecmis               â†’ son 20 kaydÄ± gÃ¶ster
  /gecmis 50            â†’ son 50 kaydÄ± gÃ¶ster
  /exportmd             â†’ geÃ§miÅŸi Markdown olarak kaydet
  /kaydet ac|kapat      â†’ dosyaya loglamayÄ± aÃ§/kapat
  /silgecmis            â†’ geÃ§miÅŸi ve dosyayÄ± sil
  /ogret TERIM: TANIM   â†’ yeni bir terim ve tanÄ±mÄ± Ã¶ÄŸret (kalÄ±cÄ± kaydetmeye Ã§alÄ±ÅŸÄ±r)
  /ogren TERIM          â†’ etkileÅŸimli olarak TERIM'i Ã¶ÄŸren (hÄ±zlÄ±)
  /ogren TERIM: TANIM   â†’ doÄŸrudan TERIM'i kaydet
  /yardim               â†’ bu menÃ¼
  (komut deÄŸilse normal soru olarak iÅŸlenir)
"""

# === Generatif Model (yÃ¼kleme + retrieval + Ã¼retim) ===
BASE_DIR = Path(__file__).resolve().parent

def _resolve_ckpt():
    # 1) DOMAIN_LLM_CKPT env
    env_p = os.environ.get('DOMAIN_LLM_CKPT', '').strip()
    if env_p and os.path.exists(env_p):
        return env_p
    # 2) Local next to this script
    local = BASE_DIR / 'gpt_gen_checkpoint.pth'
    if local.exists():
        return str(local)
    # 3) CWD
    cwd = Path.cwd() / 'gpt_gen_checkpoint.pth'
    if cwd.exists():
        return str(cwd)
    # 4) Colab default
    colab = Path('/content/gpt_gen_checkpoint.pth')
    if colab.exists():
        return str(colab)
    return ''

CKPT_PATH = _resolve_ckpt()
# Opsiyonel genel model (alan dÄ±ÅŸÄ± sorular iÃ§in)
CKPT_GENERAL = os.environ.get('GENERAL_LLM_CKPT', '').strip()
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = None
model_general = None
itos = {}
stoi = {}
encode = None
decode = None
pad_token = '<pad>'
sep_token = '</s>'
block_size = 256

def _load_ckpt(path):
    global itos, stoi, pad_token, sep_token, block_size
    if not path or not os.path.exists(path):
        return None
    ckpt = torch.load(path, map_location=device)
    itos = ckpt['itos']; stoi = ckpt['stoi']
    cfg = ckpt.get('config', {})
    block_size = cfg.get('block_size', 256)
    n_embd  = cfg.get('n_embd', 512)
    n_layer = cfg.get('n_layer', 8)
    n_head  = cfg.get('n_head', 8)
    dropout = cfg.get('dropout', 0.1)
    pad_token = cfg.get('pad_token', pad_token)
    sep_token = cfg.get('sep_token', sep_token)

    model_local = GPTLanguageModel(
        vocab_size=len(itos),
        block_size=block_size,
        n_embd=n_embd,
        n_layer=n_layer,
        n_head=n_head,
        dropout=dropout
    ).to(device)
    model_local.load_state_dict(ckpt['model_state_dict'])
    model_local.eval()
    return model_local

def load_generative():
    global model, model_general, itos, stoi, encode, decode
    model = _load_ckpt(CKPT_PATH)
    # Genel model ayrÄ± checkpoint ise yÃ¼kle
    if CKPT_GENERAL and os.path.exists(CKPT_GENERAL):
        model_general = _load_ckpt(CKPT_GENERAL)
    else:
        model_general = None
    # Only set enc/dec if a model is loaded and vocab exists
    if model is not None and isinstance(stoi, dict) and stoi:
        def _enc_safe(s):
            any_idx = next(iter(stoi.values()))
            return [stoi.get(c, any_idx) for c in s]
        def _dec_safe(l):
            return ''.join(itos.get(i, '') for i in l)
        encode = _enc_safe
        decode = _dec_safe
    else:
        encode = None
        decode = None
    return model is not None

_ = load_generative()

def serialize_record(name, rec):
    parts = [f"{canon_name(name)}:"]
    for k, v in rec.items():
        parts.append(f"{k}: {v}")
    return " ".join(parts)

def top_k_records(query, k=8):
    nq = norm(query)
    hits = set()
    # 0) Sorguda aÃ§Ä±k terimler varsa Ã¶ncelik ver
    terms_in_q = extract_terms_raw(query)
    if terms_in_q:
        for t in terms_in_q:
            hits.add(canon_name(t))
        return list(sorted(hits))[:k]
    # 1) Ã–zellik/kategori eÅŸleÅŸmesi (aliasâ€™larla)
    cat_root = extract_feature_from_query(query)
    if cat_root:
        keys = [norm(cat_root)] + [norm(s) for s in FEATURE_ALIASES.get(cat_root, [])]
        for k in keys:
            hits |= set(feature2terms.get(k, set()))
    # 2) kaba anahtar kelime eÅŸleÅŸmesi
    toks = [t for t in re.split(r'[\s,;:]+', nq) if t]
    for name, rec in props.items():
        hay = norm(name) + ' ' + norm(' '.join(rec.keys())) + ' ' + norm(' '.join(rec.values()))
        if any(t in hay for t in toks):
            hits.add(canon_name(name))
    return list(sorted(hits))[:k]

def build_context(cands):
    lines = []
    for name, rec in props.items():
        base = canon_name(name)
        if base in cands:
            lines.append(serialize_record(name, rec))
    return " | ".join(lines)

@torch.no_grad()
def generate_text(prompt, max_new_tokens=200, temperature=0.9, top_k=0):
    if (model is None) or (encode is None) or (decode is None):
        return None
    start = "<s>" + prompt
    idx = torch.tensor([encode(start)], dtype=torch.long).to(device)
    generated_tail = ""
    for _ in range(max_new_tokens):
        idx_cond = idx[:, -block_size:]
        logits, _ = model(idx_cond)
        logits = logits[:, -1, :]
        if top_k > 0:
            v, _ = torch.topk(logits, top_k)
            logits[logits < v[:, [-1]]] = -float('Inf')
        probs = torch.softmax(logits / max(1e-5, temperature), dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)
        idx = torch.cat([idx, next_token], dim=1)
        # incremental decode of just the last char
        ch = globals().get('itos', {}).get(int(next_token.item()), '')
        generated_tail += ch
        # Erken durdurma: ayraÃ§ Ã¼retildiÄŸinde kes
        if sep_token and sep_token in generated_tail:
            break
    full = decode(idx[0].tolist())
    return full[len("<s>"):]

def _extract_answer_only(generated_text: str) -> str:
    if not generated_text:
        return ""
    # Son 'Cevap:' iÅŸaretinden sonrasÄ±nÄ± al
    lower = generated_text.lower()
    key = 'cevap:'
    pos = lower.rfind(key)
    ans = generated_text[pos+len(key):] if pos != -1 else generated_text
    # Bir sonraki ayraÃ§ gelirse oraya kadar kÄ±rp
    cut = ans.split(sep_token)[0] if sep_token in ans else ans
    return cut.strip()

def answer_generative(query):
    # --- Erken dÃ¶nÃ¼ÅŸler: alan sorularÄ±nÄ± ve Ã¶zellik listesini generatiften Ã–NCE yanÄ±tla ---
    intent, fld = detect_intent(query)
    term_one = extract_term(query)

    # Ã–ZELLÄ°K LÄ°STESÄ° (Ã¶rn: "atmaca Ã¶zellikleri")
    if intent == 'features' and term_one:
        feats_off, feats_user = get_features_list(term_one)
        if feats_off:
            return "Ã–zellikler: " + ", ".join(feats_off)
        if feats_user:
            return "Ã–zellikler (kullanÄ±cÄ±dan): " + ", ".join(feats_user)
        # yoksa fallback'e izin ver
        return ""

    # BELÄ°RLÄ° ALAN (Ã¶rn: "atmaca tÃ¼rÃ¼", "SOM menÅŸei")
    if intent == 'field' and fld and term_one:
        val, src, conflict = get_field_value(term_one, fld)
        if val:
            prefix = "(Resmi) " if src == 'official' else "(KullanÄ±cÄ±) "
            tail = f" {conflict}" if conflict else ""
            return f"{prefix}{fld}: {val}{tail}"
        return ""  # fallback'e izin ver

    # TANIM (varsa doÄŸrudan dÃ¶n)
    if intent == 'definition' and term_one:
        key_try1 = term_one; key_try2 = canon_name(term_one)
        if key_try1 in d: return d[key_try1]
        if key_try2 in d: return d[key_try2]
        ename, ptxt = props_for_entity(term_one)
        if ptxt: return ptxt

    # KATEGORÄ° / LÄ°STELEME
    if intent == 'feature_list':
        items = list_by_feature(query)
        if wants_count(query):
            return (f" Toplam: {len(items)}\n" + ("\n".join(f"- {n}" for n in items) if items else ""))
    # Genel liste
    if intent == 'list':
        items = list_by_keyword(query)
        if wants_count(query):
            return (f" Toplam: {len(items)}\n" + ("\n".join(f"- {n}" for n in items) if items else ""))

    # --- Generatif Ã¼retim (retrieval ile) ---
    if model is None:
        return None
    cands = top_k_records(query, k=8)
    ctx = build_context(cands)
    prompt = (f"Soru: {query} {sep_token} BaÄŸlam: {ctx} {sep_token} Cevap: "
              f"LÃ¼tfen mÃ¼mkÃ¼nse tek satÄ±rda Ã¶zetle. Gerekirse ÅŸu biÃ§imi kullan: "
              f"TanÄ±m: ... | TÃ¼r: ... | MenÅŸei: ... | Ã–zellikler: a, b, c")
    raw = generate_text(prompt, max_new_tokens=220, temperature=0.85, top_k=50)
    ans_only = _extract_answer_only(raw)

    # GÃ¼Ã§lÃ¼ fallback
    if len(ans_only) < 5:
        if term_one and term_one in d:
            return d[term_one]
        ename, ptxt = props_for_entity(term_one) if term_one else (None, None)
        if ptxt:
            return ptxt
        lst = list_by_feature(query)
        if lst:
            if wants_count(query):
                return f"ğŸ”¢ Toplam: {len(lst)}\n" + "\n".join(f"- {n}" for n in lst)
            return "\n".join(f"- {n}" for n in lst)
    return ans_only or raw

def answer_general(query):
    # Alan dÄ±ÅŸÄ± sorular iÃ§in genel model; baÄŸlam vermeden direkt Ã¼retim
    if model_general is None:
        return None
    start = f"Soru: {query} {sep_token} Cevap: "
    raw = generate_text(start, max_new_tokens=200, temperature=0.9, top_k=50)
    return _extract_answer_only(raw)

# --- CLI dÃ¶ngÃ¼sÃ¼ ---
print("ğŸ›°ï¸ SÃ¶zlÃ¼k + Ã–zellik arayÃ¼zÃ¼ yÃ¼klendi. Ã‡Ä±kmak iÃ§in Ctrl+C / Ctrl+D.")
print(HELP_TEXT)
print("ğŸ¤– Generatif model: " + ("yÃ¼klendi." if model is not None else "bulunamadÄ±, kural tabanlÄ± mod kullanÄ±lacak."))

while True:
    try:
        q = input(" Prompt Girin: ")
    except (EOFError, KeyboardInterrupt):
        print("\n GÃ¶rÃ¼ÅŸmek Ã¼zere!")
        break

    if not q.strip():
        continue

    if q.startswith('/'):
        parts = q.strip().split()
        cmd = parts[0].lower()
        if cmd in ('/yardim', '/help'):
            print(HELP_TEXT); continue
        if cmd in ('/gecmis', '/geÃ§miÅŸ'):
            n = 20
            if len(parts) > 1 and parts[1].isdigit():
                n = int(parts[1])
            print_history(n); continue
        if cmd == '/exportmd':
            export_markdown(); continue
        if cmd == '/kaydet':
            if len(parts) > 1 and parts[1].lower() in ('ac','aÃ§','on'):
                toggle_save(True)
            elif len(parts) > 1 and parts[1].lower() in ('kapat','off'):
                toggle_save(False)
            else:
                print("KullanÄ±m: /kaydet ac | /kaydet kapat")
            continue
        if cmd == '/ogret':
            # kalan tÃ¼m mesajÄ± al ve Ã¶ÄŸret
            payload = q[len(parts[0]):].strip()
            ok, msg = teach_definition_inline(payload)
            print(msg)
            continue
        if cmd == '/ogren':
            payload = q[len(parts[0]):].strip()
            # DoÄŸrudan "TERIM: TANIM" verildiyse /ogret gibi iÅŸle
            if ':' in payload:
                ok, msg = teach_definition_inline(payload)
                print(msg); continue
            # Sadece terim verildiyse hÄ±zlÄ± etkileÅŸimli giriÅŸ
            term = payload.strip()
            if not term:
                print("KullanÄ±m: /ogren TERIM  veya  /ogren TERIM: TANIM"); continue
            print(f"â†³ {term} iÃ§in kÄ±sa tanÄ±m girin (boÅŸ bÄ±rakÄ±lÄ±rsa iptal):")
            try:
                tan = input("> TanÄ±m: ").strip()
            except (EOFError, KeyboardInterrupt):
                print("Ä°ptal."); continue
            if not tan:
                print("Ä°ptal."); continue
            ok, msg = teach_definition_inline(f"{term}: {tan}")
            print(msg); continue
        if cmd == '/silgecmis':
            clear_history(); continue
        print(" Bilinmeyen komut. /yardim yazabilirsin.")
        continue

    # Ã‡oklu terim tespiti: tanÄ±m/Ã¶zellik/alan niyetlerinde Ã¶nce kullanÄ±cÄ±dan seÃ§im iste
    intent, _fld = detect_intent(q)
    terms_in_q = extract_terms_raw(q)

    # --- Coreference patch: tekil terim bulunduysa geÃ§erli konuyu gÃ¼ncelle ---
    if len(terms_in_q) == 1:
        set_current_subject(terms_in_q[0])

    if len(terms_in_q) > 1 and intent in ('definition','features','field'):
        print(" Birden fazla aday buldum, hangisini kastettiniz?\n- " + "\n- ".join(terms_in_q) + "\n")
        log_event(q, terms_in_q, None, candidates=terms_in_q)
        continue

    # Bilinmeyen terim/alanlarÄ± aktif Ã¶ÄŸren: eÄŸer resmi+Ã¶ÄŸrenilmiÅŸte yoksa kullanÄ±cÄ±dan iste
    # (YalnÄ±zca tanÄ±m/Ã¶zellik/tÃ¼r/menÅŸei niyetlerinde)
    intent, field_name = detect_intent(q)
    terms_in_q = extract_terms_raw(q)
    if len(terms_in_q) == 1 and intent in ('definition','features','field'):
        t = terms_in_q[0]
        if intent == 'definition':
            has_def = (t in d) or ('TanÄ±m' in learned_props.get(t, {}))
            if not has_def:
                print(f" {t} iÃ§in tanÄ±m kayÄ±tlÄ± deÄŸil. KÄ±sa tanÄ±mÄ±nÄ±, tÃ¼rÃ¼nÃ¼, menÅŸeini ve 2-3 Ã¶nemli Ã¶zelliÄŸini yazar mÄ±sÄ±n?")
        elif intent == 'features':
            off, usr = get_features_list(t)
            if not off and not usr:
                print(f"{t} iÃ§in Ã¶zellik bulunmadÄ±. 2-3 temel Ã¶zelliÄŸini yazar mÄ±sÄ±n?")
        elif intent == 'field' and field_name in ('TÃ¼r','MenÅŸei'):
            val, src, _ = get_field_value(t, field_name)
            if not val:
                print(f"{t} iÃ§in {field_name} bilgisi yok. {field_name} nedir?")

    # Ã–nce generatif cevap dene (artÄ±k alan/Ã¶zellik sorularÄ±nda erken dÃ¶nÃ¼ÅŸ veriyor)
    gen = answer_generative(q)
    if gen and gen.strip():
        # --- Coreference patch: generatif cevapta da tekil terimi gÃ¼ncelle ---
        one = extract_term(q)
        if one:
            set_current_subject(one)
        print(f"\n CEVAP (LLM):\n{gen}\n")
        log_event(q, None, gen)
        continue
    # Alan dÄ±ÅŸÄ± ise genel model dene
    gen2 = answer_general(q)
    if gen2 and gen2.strip():
        one = extract_term(q)
        if one:
            set_current_subject(one)
        print(f"\n CEVAP (GENEL LLM):\n{gen2}\n")
        log_event(q, None, gen2)
        continue

    # Generatif yoksa kural tabanlÄ±ya dÃ¼ÅŸ
    def answer_for(q: str):
        last_subject = None  # local use
        intent, field = detect_intent(q)
        terms_in_q = extract_terms_raw(q)  # Ã§oklu aday kontrolÃ¼
        term = terms_in_q[0] if len(terms_in_q) == 1 else None
        want_cnt = wants_count(q)

        # Ã‡oklu aday
        if len(terms_in_q) > 1 and intent in ('definition','features','field'):
            return terms_in_q, None

        # sÄ±nÄ±flandÄ±rma
        if intent == 'classify':
            include_lists = any(h in norm(q) for h in LIST_HINTS)
            text = " Ã–zelliklere gÃ¶re daÄŸÄ±lÄ±m:\n" + summarize_feature_counts(include_lists=include_lists or want_cnt)
            return None, text

        if intent == 'type_group':
            txt = summarize_types(feature_filter_root=None)
            return None, txt

        if intent == 'feature_list':
            found = list_by_feature(q)
            if want_cnt:
                return None, (f" Toplam: {len(found)}\n" + ("\n".join(f"- {n}" for n in found) if found else ""))
            if found:
                return None, "\n".join(f"- {n}" for n in found)
            else:
                return None, " Bu kategoriyle eÅŸleÅŸen kayÄ±t bulunamadÄ±."

        if intent == 'list':
            found = list_by_keyword(q)
            if want_cnt:
                return None, (f" Toplam: {len(found)}\n" + ("\n".join(f"- {n}" for n in found) if found else ""))
            if found:
                return None, "\n".join(f"- {n}" for n in found)
            else:
                return None, " Kriterle eÅŸleÅŸen bulunamadÄ±."

        if intent == 'definition':
            if term:
                key_try1 = term
                key_try2 = canon_name(term) if term else None
                ans = d.get(key_try1) or (d.get(key_try2) if key_try2 else None)
                if ans:
                    return key_try1 if key_try1 in d else key_try2, ans
                lp = learned_props.get(term, {})
                if 'TanÄ±m' in lp:
                    return term, f"(KullanÄ±cÄ±) TanÄ±m: {lp['TanÄ±m']}"
            cat = extract_feature_from_query(q)
            if cat and cat in FEATURE_DEFINITIONS and not term:
                return cat.title(), FEATURE_DEFINITIONS[cat]
            if STRICT_DEFINITION:
                if term:
                    ask = f"{term}â€™in kÄ±sa tanÄ±mÄ±, tÃ¼rÃ¼, menÅŸei ve 2-3 Ã¶nemli Ã¶zelliÄŸi nedir?"
                else:
                    ask = "BahsettiÄŸiniz terimin kÄ±sa tanÄ±mÄ±, tÃ¼rÃ¼, menÅŸei ve 2-3 Ã¶nemli Ã¶zelliÄŸi nedir?"
                return term, f" TanÄ±m kaydÄ± bulunamadÄ±. {ask} (CevabÄ±nÄ±zÄ± Ã¶ÄŸrendikten sonra kullanacaÄŸÄ±m.)"
            ename, ptxt = props_for_entity(term) if term else (None, None)
            if ename and ptxt:
                return ename, f"TanÄ±m bulunamadÄ±; kÄ±sa Ã¶zet: {ptxt}"

        if intent == 'features':
            if term is None:
                return None, " Hangi varlÄ±ÄŸÄ±n Ã¶zellikleri? (Ã–rn: 'UMTAS Ã¶zellikleri')"
            feats_off, feats_user = get_features_list(term)
            if feats_off:
                note = ""
                if feats_user:
                    extra = [f for f in feats_user if f not in feats_off]
                    if extra:
                        note = f"\n(Not: Oturumda eklenenler: {', '.join(extra)})"
                return term, ("Ã–zellikler:\n" + "\n".join(f"- {f}" for f in feats_off) + note)
            if feats_user:
                return term, ("Ã–zellikler (kullanÄ±cÄ±dan):\n" + "\n".join(f"- {f}" for f in feats_user))
            return None, " Ã–zellik bilgisi bulunamadÄ±. '/ogret TERIM: Ã–zellik1, Ã–zellik2, ...' ile ekleyebilirsin."

        if intent == 'field' and field:
            if term is None:
                return None, f" Hangi varlÄ±k iÃ§in '{field}'?"
            val, src, conflict = get_field_value(term, field)
            if val:
                prefix = "(Resmi) " if src == 'official' else "(KullanÄ±cÄ±) "
                tail = f" {conflict}" if conflict else ""
                return term, f"{prefix}{field}: {val}{tail}"

        if term and term in d and not (intent in ('features','field','definition')):
            ans = d.get(term)
            if ans:
                return term, ans

        ename, ptxt = props_for_entity(term) if term else (None, None)
        if ptxt:
            return ename, ptxt

        return term, d.get(term)

    term, ans = answer_for(q)

    if term is None and ans is None:
        print(" TanÄ±m/Ã¶zellik bulunamadÄ±.\n")
        log_event(q, term, None)
        continue

    if isinstance(term, list):
        print(" Birden fazla aday buldum, hangisini kastettiniz?\n- " + "\n- ".join(term) + "\n")
        log_event(q, term, None, candidates=term)
        continue

    if ans:
        # --- Coreference patch: baÅŸarÄ±yla cevaplanan tekil terimi geÃ§erli konu yap ---
        if isinstance(term, str) and term.strip():
            set_current_subject(term)
        print(f"\n CEVAP: {term if term else '(liste)'}\n{ans}\n")
        log_event(q, term, ans)
    else:
        print("TanÄ±m/Ã¶zellik bulunamadÄ±.\n")
        log_event(q, term, None)

